# -*- coding: utf-8 -*-
"""Qianchi Huang CS146_Winter2022_PS2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZgquGtngawhAMMp7iCJCqyXL1IaXOKmh
"""

# This code was adapted from course material by Jenna Wiens (UMichigan).

import sys

# To add your own Drive Run this cell.
from google.colab import drive
drive.mount('/content/gdrive')

train_path = '/content/gdrive/My Drive/regression_train.csv'
test_path = '/content/gdrive/My Drive/regression_test.csv'
=

import os
import numpy as np
import matplotlib.pyplot as plt
import time

######################################################################
# classes
######################################################################

class Data :
    
    def __init__(self, X=None, y=None) :
        """
        Data class.
        
        Attributes
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        """
        
        # n = number of examples, d = dimensionality
        self.X = X
        self.y = y
    
    def load(self, filename) :
        """
        Load csv file into X array of features and y array of labels.
        
        Parameters
        --------------------
            filename -- string, filename
        """
        
        # load data
        with open(filename, 'r') as fid :
            data = np.loadtxt(fid, delimiter=",")
        
        # separate features and labels
        self.X = data[:,:-1]
        self.y = data[:,-1]
    
    def plot(self, **kwargs) :
        """Plot data."""
        
        if 'color' not in kwargs :
            kwargs['color'] = 'b'
        
        fig = plt.figure(figsize=(10, 5))
        plt.scatter(self.X, self.y, **kwargs)
        plt.xlabel('x', fontsize = 16)
        plt.ylabel('y', fontsize = 16)
        plt.show()

# wrapper functions around Data class
def load_data(filename) :
    data = Data()
    data.load(filename)
    return data

def plot_data(X, y, **kwargs) :
    data = Data(X, y)
    data.plot(**kwargs)

def plot_erms(mrange, train_errs, test_errs):
    fig = plt.figure(figsize=(10, 5))
    plt.plot(mrange, train_errs, 'o-', color='red', label='Training')
    plt.plot(mrange, test_errs, 'o-', color='blue', label='Test')
    plt.xlabel(r'$m$', fontsize = 16)
    plt.ylabel(r'$E_{RMS}$', fontsize = 16)
    plt.title('Polynomial regression error')
    plt.legend()
    plt.show()

class PolynomialRegression() :
    
    def __init__(self, m=1) :
        """
        Ordinary least squares regression.
        
        Attributes
        --------------------
            coef_   -- numpy array of shape (d,)
                       estimated coefficients for the linear regression problem
            m_      -- integer
                       order for polynomial regression
        """
        self.coef_ = None
        self.m_ = m
    
    
    def generate_polynomial_features(self, X) :
        """
        Maps X to an mth degree feature vector e.g. [1, X, X^2, ..., X^m].
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,1), features
        
        Returns
        --------------------
            Phi     -- numpy array of shape (n,(m+1)), mapped features
        """
        
        n,d = X.shape
        
        # modify to create matrix for simple linear model
        
        Phi = np.ones_like(X)
        m = self.m_
        #Phi = np.append(Phi,X,axis=1)

        # modify to create matrix for polynomial model
        for i in range(1, m+1):
          Phi = np.append(Phi, X ** i, axis=1)
        
        return Phi
    
    
    def fit_GD(self, X, y, eta=None,
                eps=0, tmax=10000, verbose=False) :
        """
        Finds the coefficients of a {d-1}^th degree polynomial
        that fits the data using least squares batch gradient descent.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
            eta     -- float, step size
            eps     -- float, convergence criterion
            tmax    -- integer, maximum number of iterations
            verbose -- boolean, for debugging purposes
        
        Returns
        --------------------
            self    -- an instance of self
        """
        
        if verbose :
            plt.subplot(1, 2, 2)
            plt.xlabel('iteration')
            plt.ylabel(r'$J(\theta)$')
            plt.ion()
            plt.show()
        
        X = self.generate_polynomial_features(X) # map features
        n,d = X.shape
        eta_input = eta
        self.coef_ = np.zeros(d)                 # coefficients
        err_list  = np.zeros((tmax,1))           # errors per iteration
        # GD loop
        for t in range(tmax) :
            # update step size
            # change the default eta in the function signature to 'eta=None'
            # and update the line below to your learning rate function
            if eta_input is None :
                eta = 1/(1+t)
            else :
                eta = eta_input

            
            # track error
    
            gradient = 0
            for i in range(n):
                error = np.dot(X[i], self.coef_) - y[i]
                gradient += error * X[i].T
            self.coef_ -= 2* eta* gradient
            y_pred = np.dot(X, self.coef_) # change this line
            err_list[t] = np.sum(np.power(y - y_pred, 2)) / float(n)
            
            # stop?
            if t > 0 and abs(err_list[t] - err_list[t-1]) <= eps :
                break
            
            # debugging
            if verbose :
                x = np.reshape(X[:,1], (n,1))
                cost = self.cost(x,y)
                plt.subplot(1, 2, 1)
                plt.cla()
                plot_data(x, y)
                self.plot_regression()
                plt.subplot(1, 2, 2)
                plt.plot([t+1], [cost], 'bo')
                plt.suptitle('iteration: %d, cost: %f' % (t+1, cost))
                plt.draw()
                plt.pause(0.05) # pause for 0.05 sec
         """
        print("Stepsize:"+ str(eta))
        print('Number of iterations: %d' % (t+1))
        print("Time: " + str(time.time() -start)+" seconds")
        print("Coefficients: " + str(self.coef_))
"""
        return self
    
    
    def fit(self, X, y) :
        """
        Finds the coefficients of a {d-1}^th degree polynomial
        that fits the data using the closed form solution.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
            
        Returns
        --------------------        
            self    -- an instance of self
        """
        
        X = self.generate_polynomial_features(X) # map features

        # implement closed-form solution

        self.coef_= np.dot(np.dot(np.linalg.pinv(np.dot(X.T,X)),X.T),y)
        #print("Closed-form coefficient: " + str(self.coef_))
        return self

    
    
    def predict(self, X) :
        """
        Predict output for X.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
        
        Returns
        --------------------
            y       -- numpy array of shape (n,), predictions
        """
        if self.coef_ is None :
            raise Exception("Model not initialized. Perform a fit first.")
        
        X = self.generate_polynomial_features(X) # map features

        # predict y
        y = np.dot(X, self.coef_)

        
        return y
    
    
    def cost(self, X, y) :
        """
        Calculates the objective function.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        
        Returns
        --------------------
            cost    -- float, objective J(theta)
        """

        # compute J(theta)
        pred = self.predict(X)
        theta = (y-pred)**2
        cost = np.sum(theta)

        return cost
    
    
    def rms_error(self, X, y) :
        """
        Calculates the root mean square error.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        
        Returns
        --------------------
            error   -- float, RMSE
        """
        # compute RMSE
        n,d = X.shape
        error = np.sqrt(self.cost(X,y)/n)
        return error
    
    
    def plot_regression(self, xmin=0, xmax=1, n=50, **kwargs) :
        """Plot regression line."""
        if 'color' not in kwargs :
            kwargs['color'] = 'r'
        if 'linestyle' not in kwargs :
            kwargs['linestyle'] = '-'
        
        X = np.reshape(np.linspace(0,1,n), (n,1))
        y = self.predict(X)
        plot_data(X, y, **kwargs)
        plt.show()

######################################################################
# main
######################################################################

def main():
    # load data
    train_data = load_data(train_path)
    test_data = load_data(test_path)
    
  

    # main code for visualizations
    print('Visualizing data...')
    plot_data(train_data.X,train_data.y)
    plot_data(test_data.X,test_data.y)
    
    # main code for linear regression
    print('Investigating linear regression...')
    model = PolynomialRegression()
    model.coef_ = np.zeros(2)
    #model.fit_GD(train_data.X, train_data.y, eta=0.000001)
    #model.fit_GD(train_data.X, train_data.y, eta=0.00001)
    #model.fit_GD(train_data.X, train_data.y, eta=0.001)
    #model.fit_GD(train_data.X, train_data.y, eta=0.05)
    #model.fit(train_data.X, train_data.y)
    model.fit_GD(train_data.X, train_data.y)
    model.cost(train_data.X, train_data.y)

    # main code for polynomial regression
    print('Investigating polynomial regression...')
    train_error = list()
    test_error = list()
    for j in range(11):
      model = PolynomialRegression(m = j)
      model.fit(train_data.X, train_data.y)  
      train_error.append(model.rms_error(train_data.X, train_data.y))
      test_error.append(model.rms_error(test_data.X, test_data.y)) 
    plot_erms([j for j in range(11)], train_error, test_error)


    
    
    print("Done!")

if __name__ == "__main__":
    main()

